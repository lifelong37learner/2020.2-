{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_635azvh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B4BA5B0FCB884B6DBA6FFE071318505E","mdEditEnable":false},"source":"# 文本预处理\n\n\n文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：\n\n1. 读入文本\n2. 分词\n3. 建立字典，将每个词映射到一个唯一的索引（index）\n4. 将文本从词的序列转换为索引的序列，方便输入模型"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_da72pg7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C3B3705CB33847A895AD10619F23E97B","mdEditEnable":false},"source":"## 读入文本\n\n我们用一部英文小说，即H. G. Well的[Time Machine](http://www.gutenberg.org/ebooks/35)，作为示例，展示文本预处理的具体过程。"},{"cell_type":"code","execution_count":13,"metadata":{"graffitiCellId":"id_ytfpat1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7FA4C53DED4F42279EA3AB3229B88DB7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"# sentences 3221\n","name":"stdout"}],"source":"import collections\nimport re\n\ndef read_time_machine():\n    with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f:\n        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n    return lines\n\n\nlines = read_time_machine()\nprint('# sentences %d' % len(lines))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_gy3tram","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EABE813C62FC4E1B8DFFD7B819C31829","mdEditEnable":false},"source":"## 分词\n\n我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。"},{"cell_type":"code","execution_count":14,"metadata":{"graffitiCellId":"id_z5grfxp","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F80F8AFC1C0A48BDB66D52A18DC3A940","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], ['']]"},"transient":{},"execution_count":14}],"source":"def tokenize(sentences, token='word'):\n    \"\"\"Split sentences into word or char tokens\"\"\"\n    if token == 'word':\n        return [sentence.split(' ') for sentence in sentences]\n    elif token == 'char':\n        return [list(sentence) for sentence in sentences]\n    else:\n        print('ERROR: unkown token type '+token)\n\ntokens = tokenize(lines)\ntokens[0:2]"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_rap2ka4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"01CE759264D84FAA8C60CE9156B86157","mdEditEnable":false},"source":"## 建立字典\n\n为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。"},{"cell_type":"code","execution_count":15,"metadata":{"attributes":{"classes":[],"id":"","n":"9"},"graffitiCellId":"id_wapwqkb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"37532FBF89C242A1805534BBE05C343A","collapsed":false,"scrolled":false},"outputs":[],"source":"class Vocab(object):\n    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n        counter = count_corpus(tokens)  # : \n        self.token_freqs = list(counter.items())\n        self.idx_to_token = []\n        if use_special_tokens:\n            # padding, begin of sentence, end of sentence, unknown\n            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n            self.idx_to_token += ['', '', '', '']\n        else:\n            self.unk = 0\n            self.idx_to_token += ['']\n        self.idx_to_token += [token for token, freq in self.token_freqs\n                        if freq >= min_freq and token not in self.idx_to_token]\n        self.token_to_idx = dict()\n        for idx, token in enumerate(self.idx_to_token):\n            self.token_to_idx[token] = idx\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\ndef count_corpus(sentences):\n    tokens = [tk for st in sentences for tk in st]\n    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_k17qg7x","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DB6949BC67FF4C7481DFFD00FE64BE56","mdEditEnable":false},"source":"我们看一个例子，这里我们尝试用Time Machine作为语料构建字典"},{"cell_type":"code","execution_count":16,"metadata":{"attributes":{"classes":[],"id":"","n":"23"},"graffitiCellId":"id_hm9y6bm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1BE94FF518DB4C4A8CDFB95C0262B47E","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n","name":"stdout"}],"source":"vocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[0:10])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_l6pjfl7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"73D0F629056F41B59D4A53F15BFAB552","mdEditEnable":false},"source":"## 将词转为索引\n\n使用字典，我们可以将原文本中的句子从单词序列转换为索引序列"},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_k48bsl2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9FBB71C21B5C4F5283C70CFE3BB07112","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\nindices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\nwords: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\nindices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n","name":"stdout"}],"source":"for i in range(8, 10):\n    print('words:', tokens[i])\n    print('indices:', vocab[tokens[i]])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_q6fupul","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EA20BC8762A74B188D3FDD761BFEDE98","mdEditEnable":false},"source":"## 用现有工具进行分词\n\n我们前面介绍的分词方式非常简单，它至少有以下几个缺点:\n\n1. 标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了\n2. 类似“shouldn't\", \"doesn't\"这样的词会被错误地处理\n3. 类似\"Mr.\", \"Dr.\"这样的词会被错误地处理\n\n我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：[spaCy](https://spacy.io/)和[NLTK](https://www.nltk.org/)。\n\n下面是一个简单的例子："},{"cell_type":"code","execution_count":18,"metadata":{"graffitiCellId":"id_7u3knll","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"46F5F57611E248ECB51F04BD0104E278","collapsed":false,"scrolled":false},"outputs":[],"source":"text = \"Mr. Chen doesn't agree with my suggestion.\""},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ae3i5g2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7D5831E3D5AD4FF48155334F73065451","mdEditEnable":false},"source":"spaCy:"},{"cell_type":"code","execution_count":23,"metadata":{"graffitiCellId":"id_uz6civu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"30D69C6B1BE44362BA556E2E5EEF493A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nRequirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.2.3)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.17.2)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (41.0.1)\nRequirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (7.3.1)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.1)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.6.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.22.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.32.2)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\", line 160, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw)\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py\", line 80, in create_connection\n    raise err\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py\", line 70, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 603, in urlopen\n    chunked=chunked)\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 344, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 843, in _validate_conn\n    conn.connect()\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\", line 316, in connect\n    conn = self._new_conn()\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\", line 169, in _new_conn\n    self, \"Failed to establish a new connection: %s\" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7fee34d9e860>: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n    timeout=timeout\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 641, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/opt/conda/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fee34d9e860>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.7/site-packages/spacy/__main__.py\", line 33, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/opt/conda/lib/python3.7/site-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/opt/conda/lib/python3.7/site-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/spacy/cli/download.py\", line 44, in download\n    shortcuts = get_json(about.__shortcuts__, \"available shortcuts\")\n  File \"/opt/conda/lib/python3.7/site-packages/spacy/cli/download.py\", line 95, in get_json\n    r = requests.get(url)\n  File \"/opt/conda/lib/python3.7/site-packages/requests/api.py\", line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/requests/api.py\", line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/requests/sessions.py\", line 533, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/requests/adapters.py\", line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fee34d9e860>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-0698385a7182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."]}],"source":"!pip install spacy\nimport spacy\n!python -m spacy download en_core_web_sm\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nprint([token.text for token in doc])"},{"metadata":{"id":"765CC9B5A1C348A58A2B340ADC532FD1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"NLTK:"},{"cell_type":"code","execution_count":11,"metadata":{"graffitiCellId":"id_r13iwga","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B83D30D3670B44A38527B4943BE4DBE0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting nltk\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n\u001b[K     |████████████████████████████████| 1.5MB 167kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\nBuilding wheels for collected packages: nltk\n\u001b[33m  WARNING: Building wheel for nltk failed: [Errno 13] Permission denied: '/home/jovyan/.cache/pip/wheels'\u001b[0m\nFailed to build nltk\nInstalling collected packages: nltk\n  Running setup.py install for nltk ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed nltk-3.4.5\n['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n","name":"stdout"}],"source":"! pip install nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import data\ndata.path.append('/home/kesci/input/nltk_data3784/nltk_data')\nprint(word_tokenize(text))"},{"metadata":{"id":"1A02A36C3D214F56AD3F835F9D46FD90","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"推荐系统，其中文本的预处理是不可或缺的步骤，并且是最重要的一个步骤，导师说在项目中文本的预处理就占用了70%的时间。\n对视频中所说的不同的现实场景有不同的预处理方法深有体会，视频的讲解是处理英文文本。\n而在中国处理的最多是中文文本，但是中文文本相对英文文本存在了大量的问题。\n对于自己所做的短文本的推进系统中，总结了这些中文文本存在的问题（如果大家遇到别中文文本的问题希望补充）：\n1. 中文存在词的语义比英文更为复杂\n2. 词汇少\n3. 特征稀释\n4. 分类精确率低\n\n并且在中文的文本的预处理中，个人觉得还需要做不同停用词表和无关词表。\n其中视频中lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]可以看做为停用词的处理。\n而无关词即是对场景无关紧要之词，相当于我们日常所说的废话（但并不是人为觉得的）。\n视频中提及的分词工具可以说目前也是相对英文，对于中文的分词工具目前大多数使用的是jieba"},{"metadata":{"id":"64933F4B84A4489599B8D13B6AE69897","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"文本预处理（常见，基础，不同任务特性不同预处理）\n1、读入文本\n2、分词\n3、建立字典\n4、将文本从词的序列转换为索引序列\n\n\n#建立字典，设置阈值\n#去重筛选词，特殊需求token\n#1、count_corpus统计词频，得到counter\n#2、根据需求添加特殊的token ,增删，利用空列表\n#pad:二维矩阵长度不一，短句子补token利用pad\n#bos:开始token\n#eos：结束token\n#unk：未登录词当作unk\n#3、词到索引号,建立字典，将每个token建立映射到唯一的索引\n#4、建立索引到token的映射\n"},{"metadata":{"id":"51BE0D15FA524FE39AD4DD4695B48024","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}