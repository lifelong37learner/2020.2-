{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_z2fw17x",
    "id": "64AE0FB342E9428A8DA9C734B357BBB1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# softmax和分类模型\n",
    "内容包含：\n",
    "1. softmax回归的基本概念\n",
    "2. 如何获取Fashion-MNIST数据集和读取数据\n",
    "3. softmax回归模型的从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型\n",
    "4. 使用pytorch重新实现softmax回归模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_xq1py16",
    "id": "2031A689D83B4282ABA8CC2238D7FC9B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## softmax的基本概念\n",
    "\n",
    "- 分类问题  \n",
    "一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。  \n",
    "图像中的4像素分别记为$x_1, x_2, x_3, x_4$。  \n",
    "假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1, y_2, y_3$。  \n",
    "我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。\n",
    "\n",
    "- 权重矢量  \n",
    "$$\n",
    " \\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1 \\end{aligned} \n",
    "$$\n",
    "\n",
    "$$\n",
    " \\begin{aligned} o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2 \\end{aligned} \n",
    "$$\n",
    "\n",
    "$$\n",
    " \\begin{aligned} o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \\end{aligned} \n",
    "$$\n",
    "\n",
    "- 神经网络图  \n",
    "下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5hmymezog.png)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}softmax回归是一个单层神经网络\\end{aligned}\n",
    "$$\n",
    "\n",
    "既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\\underset{i}{\\arg\\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。\n",
    "\n",
    "- 输出问题  \n",
    "直接使用输出层的输出有两个问题：\n",
    "    1. 一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。\n",
    "    2. 另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。\n",
    "\n",
    "softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：\n",
    "\n",
    "$$\n",
    " \\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = \\text{softmax}(o_1, o_2, o_3) \n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    " \\hat{y}1 = \\frac{ \\exp(o_1)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad \\hat{y}2 = \\frac{ \\exp(o_2)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad \\hat{y}3 = \\frac{ \\exp(o_3)}{\\sum_{i=1}^3 \\exp(o_i)}. \n",
    "$$\n",
    "\n",
    "容易看出$\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$且$0 \\leq \\hat{y}_1, \\hat{y}_2, \\hat{y}_3 \\leq 1$，因此$\\hat{y}_1, \\hat{y}_2, \\hat{y}_3$是一个合法的概率分布。这时候，如果$\\hat{y}_2=0.8$，不管$\\hat{y}_1$和$\\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到\n",
    "\n",
    "$$\n",
    " \\underset{i}{\\arg\\max} o_i = \\underset{i}{\\arg\\max} \\hat{y}_i \n",
    "$$\n",
    "\n",
    "因此softmax运算不改变预测类别输出。\n",
    "\n",
    "- 计算效率\n",
    "    - 单样本矢量计算表达式  \n",
    "    为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为\n",
    "   \n",
    "$$\n",
    " \\boldsymbol{W} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\\\ w_{41} & w_{42} & w_{43} \\end{bmatrix},\\quad \\boldsymbol{b} = \\begin{bmatrix} b_1 & b_2 & b_3 \\end{bmatrix}, \n",
    "$$\n",
    "\n",
    "设高和宽分别为2个像素的图像样本$i$的特征为\n",
    "   \n",
    "$$\n",
    "\\boldsymbol{x}^{(i)} = \\begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "输出层的输出为\n",
    "\n",
    "$$\n",
    "\\boldsymbol{o}^{(i)} = \\begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "预测为狗、猫或鸡的概率分布为\n",
    "    \n",
    "$$\n",
    "\\boldsymbol{\\hat{y}}^{(i)} = \\begin{bmatrix}\\hat{y}_1^{(i)} & \\hat{y}_2^{(i)} & \\hat{y}_3^{(i)}\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "softmax回归对样本$i$分类的矢量计算表达式为\n",
    "   \n",
    "$$\n",
    " \\begin{aligned} \\boldsymbol{o}^{(i)} &= \\boldsymbol{x}^{(i)} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{y}}^{(i)} &= \\text{softmax}(\\boldsymbol{o}^{(i)}). \\end{aligned} \n",
    "$$\n",
    "\n",
    "- 小批量矢量计算表达式  \n",
    "    为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。假设softmax回归的权重和偏差参数分别为$\\boldsymbol{W} \\in \\mathbb{R}^{d \\times q}$和$\\boldsymbol{b} \\in \\mathbb{R}^{1 \\times q}$。softmax回归的矢量计算表达式为\n",
    "\n",
    "$$\n",
    " \\begin{aligned} \\boldsymbol{O} &= \\boldsymbol{X} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{Y}} &= \\text{softmax}(\\boldsymbol{O}), \\end{aligned} \n",
    "$$\n",
    "\n",
    "其中的加法运算使用了广播机制，$\\boldsymbol{O}, \\boldsymbol{\\hat{Y}} \\in \\mathbb{R}^{n \\times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\\boldsymbol{o}^{(i)}$和概率分布$\\boldsymbol{\\hat{y}}^{(i)}$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "900A996DF3114CBE8545153F973B4640",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 交叉熵损失函数\n",
    "\n",
    "对于样本$i$，我们构造向量$\\boldsymbol{y}^{(i)}\\in \\mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\\boldsymbol{\\hat y}^{(i)}$尽可能接近真实的标签概率分布$\\boldsymbol{y}^{(i)}$。\n",
    "\n",
    "- 平方损失估计  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}Loss = |\\boldsymbol{\\hat y}^{(i)}-\\boldsymbol{y}^{(i)}|^2/2\\end{aligned}\n",
    "$$\n",
    "  \n",
    "\n",
    "然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\\hat{y}^{(i)}_3$比其他两个预测值$\\hat{y}^{(i)}_1$和$\\hat{y}^{(i)}_2$大就行了。即使$\\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\\hat y^{(i)}_1=\\hat y^{(i)}_2=0.2$比$\\hat y^{(i)}_1=0, \\hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。\n",
    "\n",
    "改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：\n",
    "\n",
    "\n",
    "$$\n",
    "H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},\n",
    "$$\n",
    "\n",
    "\n",
    "其中带下标的$y_j^{(i)}$是向量$\\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}{y^{(i)}}$为1，其余全为0，于是$H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}) = -\\log \\hat y{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。\n",
    "\n",
    "假设训练数据集的样本数为$n$，交叉熵损失函数定义为 \n",
    "$$\n",
    "\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ),\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\boldsymbol{\\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\\ell(\\boldsymbol{\\Theta}) = -(1/n) \\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\\ell(\\boldsymbol{\\Theta})$等价于最大化$\\exp(-n\\ell(\\boldsymbol{\\Theta}))=\\prod_{i=1}^n \\hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9221297B8C694D4F9B76FDFD29C02E5E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 模型训练和预测\n",
    "在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。在3.6节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_co5e0yp",
    "id": "3D1A30145B5A450E819DCA2FB094117B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 获取Fashion-MNIST训练集和读取数据\n",
    "在介绍softmax回归的实现前我们先引入一个多类图像分类数据集。它将在后面的章节中被多次使用，以方便我们观察比较算法之间在模型精度和计算效率上的区别。图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]。\n",
    "\n",
    "我这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：\n",
    "1. torchvision.datasets: 一些加载数据的函数及常用的数据集接口；\n",
    "2. torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；\n",
    "3. torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；\n",
    "4. torchvision.utils: 其他的一些有用的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "graffitiCellId": "id_my8ejol",
    "id": "9C804F2DABDA482BB4375CB0D36E011D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting torchtext\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.8kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext) (4.32.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.3.0)\n",
      "Collecting sentencepiece (from torchtext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 12kB/s eta 0:00:016\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.17.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n",
      "1.3.0\n",
      "0.4.1a0+d94043a\n"
     ]
    }
   ],
   "source": [
    "# import needed package\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "!pip install torchtext\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\")\n",
    "import d2lzh1981 as d2l\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_75u5u2k",
    "id": "810E6357484047F08D83319B46F7438F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_no4tgy0",
    "id": "6FF62DC73B1D46128E613FC7DFFB3C3C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 26386432/26421880 [16:06<00:00, 75235.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/29515 [00:00<?, ?it/s]\u001b[A\n",
      " 56%|█████▌    | 16384/29515 [00:01<00:00, 55385.84it/s]\u001b[A\n",
      "32768it [00:01, 55116.32it/s]                           \u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4422102 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 16384/4422102 [00:01<01:35, 46206.51it/s]\u001b[A\n",
      "  1%|          | 24576/4422102 [00:01<02:03, 35535.80it/s]\u001b[A\n",
      "  1%|▏         | 65536/4422102 [00:02<01:48, 40174.13it/s]\u001b[A\n",
      "  2%|▏         | 98304/4422102 [00:02<01:29, 48363.73it/s]\u001b[A\n",
      "  3%|▎         | 114688/4422102 [00:03<01:58, 36416.55it/s]\u001b[A\n",
      "  3%|▎         | 139264/4422102 [00:03<01:40, 42464.91it/s]\u001b[A\n",
      "  4%|▎         | 155648/4422102 [00:03<01:38, 43489.90it/s]\u001b[A\n",
      "  4%|▎         | 163840/4422102 [00:04<02:04, 34296.30it/s]\u001b[A\n",
      "  4%|▍         | 188416/4422102 [00:04<01:44, 40425.09it/s]\u001b[A\n",
      "  4%|▍         | 196608/4422102 [00:04<02:08, 32990.02it/s]\u001b[A\n",
      "  5%|▍         | 212992/4422102 [00:06<03:19, 21081.34it/s]\u001b[A\n",
      "  6%|▌         | 245760/4422102 [00:06<02:32, 27427.39it/s]\u001b[A\n",
      "  6%|▌         | 253952/4422102 [00:07<03:35, 19381.78it/s]\u001b[A\n",
      "  6%|▌         | 262144/4422102 [00:07<03:24, 20343.02it/s]\u001b[A\n",
      "  6%|▌         | 270336/4422102 [00:08<03:16, 21090.64it/s]\u001b[A\n",
      "  6%|▋         | 278528/4422102 [00:08<03:11, 21649.95it/s]\u001b[A\n",
      "  6%|▋         | 286720/4422102 [00:08<03:07, 22062.66it/s]\u001b[A\n",
      "  7%|▋         | 294912/4422102 [00:09<03:04, 22352.48it/s]\u001b[A\n",
      "  7%|▋         | 303104/4422102 [00:09<03:56, 17444.55it/s]\u001b[A\n",
      "  7%|▋         | 311296/4422102 [00:10<03:38, 18803.74it/s]\u001b[A\n",
      "  7%|▋         | 319488/4422102 [00:10<03:26, 19890.05it/s]\u001b[A\n",
      "  7%|▋         | 327680/4422102 [00:11<03:16, 20797.95it/s]\u001b[A\n",
      "  8%|▊         | 335872/4422102 [00:11<03:10, 21439.18it/s]\u001b[A\n",
      "26427392it [16:20, 75235.55it/s]                              \n",
      "  8%|▊         | 360448/4422102 [00:12<02:36, 25957.20it/s]\u001b[A\n",
      "  8%|▊         | 368640/4422102 [00:12<02:41, 25082.52it/s]\u001b[A\n",
      "  9%|▊         | 376832/4422102 [00:13<04:04, 16559.90it/s]\u001b[A\n",
      "  9%|▊         | 385024/4422102 [00:13<03:16, 20516.02it/s]\u001b[A\n",
      "  9%|▉         | 393216/4422102 [00:13<03:10, 21160.15it/s]\u001b[A\n",
      "  9%|▉         | 401408/4422102 [00:14<03:05, 21676.59it/s]\u001b[A\n",
      "  9%|▉         | 409600/4422102 [00:14<03:01, 22082.13it/s]\u001b[A\n",
      "  9%|▉         | 417792/4422102 [00:14<02:45, 24204.82it/s]\u001b[A\n",
      " 10%|▉         | 425984/4422102 [00:15<02:47, 23849.40it/s]\u001b[A\n",
      " 10%|▉         | 434176/4422102 [00:15<02:42, 24546.48it/s]\u001b[A\n",
      " 10%|█         | 442368/4422102 [00:16<03:37, 18338.93it/s]\u001b[A\n",
      " 10%|█         | 450560/4422102 [00:16<03:23, 19540.03it/s]\u001b[A\n",
      " 10%|█         | 458752/4422102 [00:18<06:41, 9870.25it/s] \u001b[A\n",
      " 11%|█         | 466944/4422102 [00:19<06:25, 10269.98it/s]\u001b[A\n",
      " 11%|█         | 475136/4422102 [00:20<07:56, 8289.83it/s] \u001b[A\n",
      " 11%|█         | 483328/4422102 [00:21<07:15, 9035.91it/s]\u001b[A\n",
      " 11%|█         | 491520/4422102 [00:21<06:06, 10715.24it/s]\u001b[A\n",
      " 11%|█▏        | 499712/4422102 [00:22<05:47, 11289.75it/s]\u001b[A\n",
      " 11%|█▏        | 507904/4422102 [00:22<05:04, 12835.15it/s]\u001b[A\n",
      " 12%|█▏        | 516096/4422102 [00:23<05:54, 11029.14it/s]\u001b[A\n",
      " 12%|█▏        | 524288/4422102 [00:24<04:58, 13077.91it/s]\u001b[A\n",
      " 12%|█▏        | 532480/4422102 [00:25<05:40, 11439.40it/s]\u001b[A\n",
      " 12%|█▏        | 540672/4422102 [00:25<05:15, 12301.58it/s]\u001b[A\n",
      " 12%|█▏        | 548864/4422102 [00:26<06:12, 10404.82it/s]\u001b[A\n",
      " 13%|█▎        | 557056/4422102 [00:27<06:51, 9400.85it/s] \u001b[A\n",
      " 13%|█▎        | 565248/4422102 [00:28<06:27, 9952.33it/s]\u001b[A\n",
      " 13%|█▎        | 573440/4422102 [00:28<05:20, 12000.43it/s]\u001b[A\n",
      " 13%|█▎        | 581632/4422102 [00:29<05:24, 11826.67it/s]\u001b[A\n",
      " 13%|█▎        | 589824/4422102 [00:29<04:36, 13854.02it/s]\u001b[A\n",
      " 14%|█▎        | 598016/4422102 [00:30<04:02, 15740.45it/s]\u001b[A\n",
      " 14%|█▎        | 606208/4422102 [00:30<03:55, 16208.20it/s]\u001b[A\n",
      " 14%|█▍        | 614400/4422102 [00:31<03:44, 16987.50it/s]\u001b[A\n",
      " 14%|█▍        | 622592/4422102 [00:31<03:00, 21048.04it/s]\u001b[A\n",
      " 14%|█▍        | 630784/4422102 [00:31<02:55, 21618.76it/s]\u001b[A\n",
      " 14%|█▍        | 638976/4422102 [00:31<02:51, 22043.79it/s]\u001b[A\n",
      " 15%|█▍        | 655360/4422102 [00:32<02:24, 26140.93it/s]\u001b[A\n",
      " 15%|█▌        | 663552/4422102 [00:32<02:29, 25128.63it/s]\u001b[A\n",
      " 15%|█▌        | 671744/4422102 [00:33<03:22, 18562.62it/s]\u001b[A\n",
      " 16%|█▌        | 688128/4422102 [00:33<02:45, 22624.60it/s]\u001b[A\n",
      " 16%|█▌        | 696320/4422102 [00:34<03:32, 17571.22it/s]\u001b[A\n",
      " 16%|█▌        | 704512/4422102 [00:35<04:53, 12687.26it/s]\u001b[A\n",
      " 16%|█▌        | 712704/4422102 [00:35<04:12, 14669.56it/s]\u001b[A\n",
      " 16%|█▋        | 720896/4422102 [00:36<03:44, 16472.32it/s]\u001b[A\n",
      " 16%|█▋        | 729088/4422102 [00:36<04:12, 14604.00it/s]\u001b[A\n",
      " 17%|█▋        | 737280/4422102 [00:37<03:44, 16405.32it/s]\u001b[A\n",
      " 17%|█▋        | 745472/4422102 [00:38<04:12, 14564.57it/s]\u001b[A\n",
      " 17%|█▋        | 753664/4422102 [00:38<03:44, 16373.32it/s]\u001b[A\n",
      " 17%|█▋        | 761856/4422102 [00:40<07:10, 8493.86it/s] \u001b[A\n",
      " 17%|█▋        | 770048/4422102 [00:41<08:12, 7422.35it/s]\u001b[A\n",
      " 18%|█▊        | 778240/4422102 [00:42<08:08, 7458.12it/s]\u001b[A\n",
      " 18%|█▊        | 786432/4422102 [00:43<06:28, 9348.99it/s]\u001b[A\n",
      " 18%|█▊        | 794624/4422102 [00:44<07:17, 8285.34it/s]\u001b[A\n",
      " 18%|█▊        | 802816/4422102 [00:45<06:20, 9502.89it/s]\u001b[A\n",
      " 18%|█▊        | 811008/4422102 [00:45<05:59, 10036.30it/s]\u001b[A\n",
      " 19%|█▊        | 819200/4422102 [00:46<05:44, 10448.05it/s]\u001b[A\n",
      " 19%|█▊        | 827392/4422102 [00:46<04:47, 12499.61it/s]\u001b[A\n",
      " 19%|█▉        | 835584/4422102 [00:47<04:24, 13573.01it/s]\u001b[A\n",
      " 19%|█▉        | 843776/4422102 [00:47<03:51, 15490.48it/s]\u001b[A\n",
      " 19%|█▉        | 851968/4422102 [00:48<03:27, 17185.41it/s]\u001b[A\n",
      " 19%|█▉        | 860160/4422102 [00:48<03:57, 14966.32it/s]\u001b[A\n",
      " 20%|█▉        | 868352/4422102 [00:49<03:15, 18166.62it/s]\u001b[A\n",
      " 20%|█▉        | 876544/4422102 [00:49<03:47, 15589.15it/s]\u001b[A\n",
      " 20%|██        | 884736/4422102 [00:50<03:24, 17274.62it/s]\u001b[A\n",
      " 20%|██        | 892928/4422102 [00:50<03:16, 17967.39it/s]\u001b[A\n",
      " 20%|██        | 901120/4422102 [00:50<03:02, 19247.94it/s]\u001b[A\n",
      " 21%|██        | 909312/4422102 [00:51<02:53, 20260.05it/s]\u001b[A\n",
      " 21%|██        | 917504/4422102 [00:51<03:32, 16514.43it/s]\u001b[A\n",
      " 21%|██        | 925696/4422102 [00:52<03:13, 18051.91it/s]\u001b[A\n",
      " 21%|██        | 933888/4422102 [00:53<03:54, 14877.68it/s]\u001b[A\n",
      " 21%|██▏       | 942080/4422102 [00:53<03:11, 18186.63it/s]\u001b[A\n",
      " 21%|██▏       | 950272/4422102 [00:53<03:07, 18484.08it/s]\u001b[A\n",
      " 22%|██▏       | 958464/4422102 [00:54<02:56, 19664.19it/s]\u001b[A\n",
      " 22%|██▏       | 966656/4422102 [00:54<02:48, 20566.76it/s]\u001b[A\n",
      " 22%|██▏       | 974848/4422102 [00:55<03:35, 16011.80it/s]\u001b[A\n",
      " 22%|██▏       | 983040/4422102 [00:55<03:06, 18423.89it/s]\u001b[A\n",
      " 22%|██▏       | 991232/4422102 [00:55<03:03, 18717.44it/s]\u001b[A\n",
      " 23%|██▎       | 999424/4422102 [00:56<04:21, 13090.43it/s]\u001b[A\n",
      " 23%|██▎       | 1015808/4422102 [00:57<03:24, 16676.87it/s]\u001b[A\n",
      " 23%|██▎       | 1024000/4422102 [00:57<03:07, 18168.91it/s]\u001b[A\n",
      " 23%|██▎       | 1032192/4422102 [00:58<02:54, 19386.09it/s]\u001b[A\n",
      " 24%|██▎       | 1040384/4422102 [00:58<03:30, 16102.73it/s]\u001b[A\n",
      " 24%|██▎       | 1048576/4422102 [00:59<03:10, 17687.67it/s]\u001b[A\n",
      " 24%|██▍       | 1056768/4422102 [00:59<03:41, 15160.09it/s]\u001b[A\n",
      " 24%|██▍       | 1064960/4422102 [01:00<03:18, 16871.08it/s]\u001b[A\n",
      " 24%|██▍       | 1073152/4422102 [01:00<03:03, 18291.95it/s]\u001b[A\n",
      " 24%|██▍       | 1081344/4422102 [01:01<03:34, 15542.13it/s]\u001b[A\n",
      " 25%|██▍       | 1089536/4422102 [01:01<03:13, 17188.05it/s]\u001b[A\n",
      " 25%|██▍       | 1097728/4422102 [01:01<02:59, 18538.18it/s]\u001b[A\n",
      " 25%|██▌       | 1105920/4422102 [01:02<03:31, 15660.88it/s]\u001b[A\n",
      " 25%|██▌       | 1114112/4422102 [01:03<03:10, 17354.98it/s]\u001b[A\n",
      " 25%|██▌       | 1122304/4422102 [01:03<02:55, 18756.65it/s]\u001b[A\n",
      " 26%|██▌       | 1130496/4422102 [01:03<02:51, 19237.64it/s]\u001b[A\n",
      " 26%|██▌       | 1138688/4422102 [01:04<02:42, 20258.21it/s]\u001b[A\n",
      " 26%|██▌       | 1146880/4422102 [01:04<02:35, 21032.28it/s]\u001b[A\n",
      " 26%|██▌       | 1155072/4422102 [01:04<02:31, 21605.71it/s]\u001b[A\n",
      " 26%|██▋       | 1163264/4422102 [01:05<02:28, 21966.18it/s]\u001b[A\n",
      " 26%|██▋       | 1171456/4422102 [01:06<03:45, 14432.15it/s]\u001b[A\n",
      " 27%|██▋       | 1187840/4422102 [01:06<03:00, 17881.43it/s]\u001b[A\n",
      " 27%|██▋       | 1196032/4422102 [01:06<02:48, 19176.81it/s]\u001b[A\n",
      " 27%|██▋       | 1204224/4422102 [01:08<04:08, 12955.29it/s]\u001b[A\n",
      " 27%|██▋       | 1212416/4422102 [01:08<03:35, 14918.82it/s]\u001b[A\n",
      " 28%|██▊       | 1220608/4422102 [01:08<03:11, 16688.64it/s]\u001b[A\n",
      " 28%|██▊       | 1228800/4422102 [01:09<02:55, 18207.30it/s]\u001b[A\n",
      " 28%|██▊       | 1236992/4422102 [01:09<02:43, 19444.92it/s]\u001b[A\n",
      " 28%|██▊       | 1245184/4422102 [01:09<02:36, 20364.21it/s]\u001b[A\n",
      " 28%|██▊       | 1253376/4422102 [01:10<02:31, 20974.34it/s]\u001b[A\n",
      " 29%|██▊       | 1261568/4422102 [01:10<02:27, 21491.10it/s]\u001b[A\n",
      " 29%|██▊       | 1269760/4422102 [01:10<02:23, 21945.63it/s]\u001b[A\n",
      " 29%|██▉       | 1286144/4422102 [01:11<02:41, 19461.51it/s]\u001b[A\n",
      " 30%|██▉       | 1310720/4422102 [01:12<02:06, 24504.29it/s]\u001b[A\n",
      " 30%|██▉       | 1318912/4422102 [01:12<02:08, 24055.77it/s]\u001b[A\n",
      " 30%|███       | 1327104/4422102 [01:13<02:05, 24622.25it/s]\u001b[A\n",
      " 30%|███       | 1335296/4422102 [01:13<02:08, 24081.95it/s]\u001b[A\n",
      " 31%|███       | 1351680/4422102 [01:13<01:49, 28051.02it/s]\u001b[A\n",
      " 31%|███       | 1359872/4422102 [01:14<01:56, 26333.56it/s]\u001b[A\n",
      " 31%|███       | 1368064/4422102 [01:14<02:00, 25280.25it/s]\u001b[A\n",
      " 31%|███▏      | 1384448/4422102 [01:15<02:06, 24106.89it/s]\u001b[A\n",
      " 31%|███▏      | 1392640/4422102 [01:15<02:03, 24574.73it/s]\u001b[A\n",
      " 32%|███▏      | 1400832/4422102 [01:15<02:05, 24038.13it/s]\u001b[A\n",
      " 32%|███▏      | 1409024/4422102 [01:16<02:11, 22911.93it/s]\u001b[A\n",
      " 32%|███▏      | 1417216/4422102 [01:16<02:06, 23768.78it/s]\u001b[A\n",
      " 32%|███▏      | 1425408/4422102 [01:16<02:07, 23562.90it/s]\u001b[A\n",
      " 32%|███▏      | 1433600/4422102 [01:17<02:07, 23408.43it/s]\u001b[A\n",
      " 33%|███▎      | 1449984/4422102 [01:17<01:48, 27446.45it/s]\u001b[A\n",
      " 33%|███▎      | 1458176/4422102 [01:18<03:15, 15136.20it/s]\u001b[A\n",
      " 33%|███▎      | 1466368/4422102 [01:19<03:58, 12418.04it/s]\u001b[A\n",
      " 33%|███▎      | 1474560/4422102 [01:20<04:02, 12136.48it/s]\u001b[A\n",
      " 34%|███▎      | 1482752/4422102 [01:21<04:06, 11927.57it/s]\u001b[A\n",
      " 34%|███▎      | 1490944/4422102 [01:21<04:08, 11808.52it/s]\u001b[A\n",
      " 34%|███▍      | 1499136/4422102 [01:22<03:31, 13835.76it/s]\u001b[A\n",
      " 34%|███▍      | 1507328/4422102 [01:22<03:43, 13055.21it/s]\u001b[A\n",
      " 34%|███▍      | 1523712/4422102 [01:23<03:13, 15014.84it/s]\u001b[A\n",
      " 35%|███▍      | 1540096/4422102 [01:24<02:33, 18816.83it/s]\u001b[A\n",
      " 35%|███▌      | 1548288/4422102 [01:24<03:01, 15825.29it/s]\u001b[A\n",
      " 35%|███▌      | 1556480/4422102 [01:25<02:43, 17480.77it/s]\u001b[A\n",
      " 35%|███▌      | 1564672/4422102 [01:26<03:43, 12796.58it/s]\u001b[A\n",
      " 36%|███▌      | 1572864/4422102 [01:27<04:27, 10668.52it/s]\u001b[A\n",
      " 36%|███▌      | 1581056/4422102 [01:27<03:43, 12728.97it/s]\u001b[A\n",
      " 36%|███▌      | 1589248/4422102 [01:28<05:03, 9335.86it/s] \u001b[A\n",
      " 36%|███▌      | 1597440/4422102 [01:30<05:31, 8518.96it/s]\u001b[A\n",
      " 36%|███▋      | 1605632/4422102 [01:30<04:28, 10508.52it/s]\u001b[A\n",
      " 36%|███▋      | 1613824/4422102 [01:31<04:20, 10787.64it/s]\u001b[A\n",
      " 37%|███▋      | 1622016/4422102 [01:32<04:51, 9613.65it/s] \u001b[A\n",
      " 37%|███▋      | 1630208/4422102 [01:32<04:38, 10011.02it/s]\u001b[A\n",
      " 37%|███▋      | 1638400/4422102 [01:33<04:27, 10423.49it/s]\u001b[A\n",
      " 37%|███▋      | 1646592/4422102 [01:34<04:18, 10734.18it/s]\u001b[A\n",
      " 37%|███▋      | 1654784/4422102 [01:35<05:10, 8903.49it/s] \u001b[A\n",
      " 38%|███▊      | 1662976/4422102 [01:36<04:48, 9557.35it/s]\u001b[A\n",
      " 38%|███▊      | 1671168/4422102 [01:37<04:33, 10071.74it/s]\u001b[A\n",
      " 38%|███▊      | 1679360/4422102 [01:37<03:46, 12117.27it/s]\u001b[A\n",
      " 38%|███▊      | 1687552/4422102 [01:38<04:09, 10954.88it/s]\u001b[A\n",
      " 38%|███▊      | 1695744/4422102 [01:38<03:09, 14401.55it/s]\u001b[A\n",
      " 39%|███▊      | 1703936/4422102 [01:39<03:10, 14302.06it/s]\u001b[A\n",
      " 39%|███▊      | 1712128/4422102 [01:40<04:51, 9301.00it/s] \u001b[A\n",
      " 39%|███▉      | 1720320/4422102 [01:43<08:19, 5407.47it/s]\u001b[A\n",
      " 39%|███▉      | 1728512/4422102 [01:44<06:23, 7020.64it/s]\u001b[A\n",
      " 39%|███▉      | 1736704/4422102 [01:45<06:14, 7167.28it/s]\u001b[A\n",
      " 39%|███▉      | 1744896/4422102 [01:45<05:29, 8126.96it/s]\u001b[A\n",
      " 40%|███▉      | 1753088/4422102 [01:46<05:35, 7952.61it/s]\u001b[A\n",
      " 40%|███▉      | 1761280/4422102 [01:47<05:03, 8753.90it/s]\u001b[A\n",
      " 40%|████      | 1769472/4422102 [01:48<04:45, 9303.44it/s]\u001b[A\n",
      " 40%|████      | 1777664/4422102 [01:49<04:23, 10025.50it/s]\u001b[A\n",
      " 40%|████      | 1785856/4422102 [01:49<04:13, 10396.83it/s]\u001b[A\n",
      " 41%|████      | 1794048/4422102 [01:50<03:31, 12450.13it/s]\u001b[A\n",
      " 41%|████      | 1802240/4422102 [01:50<03:35, 12141.36it/s]\u001b[A\n",
      " 41%|████      | 1810432/4422102 [01:52<04:48, 9062.85it/s] \u001b[A\n",
      " 41%|████▏     | 1826816/4422102 [01:52<03:54, 11083.42it/s]\u001b[A\n",
      " 41%|████▏     | 1835008/4422102 [01:53<03:17, 13111.74it/s]\u001b[A\n",
      " 42%|████▏     | 1843200/4422102 [01:53<02:51, 15008.84it/s]\u001b[A\n",
      " 42%|████▏     | 1851392/4422102 [01:54<02:33, 16715.08it/s]\u001b[A\n",
      " 42%|████▏     | 1859584/4422102 [01:54<02:25, 17635.51it/s]\u001b[A\n",
      " 42%|████▏     | 1867776/4422102 [01:54<02:22, 17961.85it/s]\u001b[A\n",
      " 42%|████▏     | 1875968/4422102 [01:55<02:29, 16998.88it/s]\u001b[A\n",
      " 43%|████▎     | 1892352/4422102 [01:56<02:19, 18090.26it/s]\u001b[A\n",
      " 43%|████▎     | 1900544/4422102 [01:56<02:11, 19222.80it/s]\u001b[A\n",
      " 43%|████▎     | 1908736/4422102 [01:57<03:04, 13652.03it/s]\u001b[A\n",
      " 44%|████▎     | 1925120/4422102 [01:58<02:57, 14071.52it/s]\u001b[A\n",
      " 44%|████▎     | 1933312/4422102 [01:59<02:36, 15898.22it/s]\u001b[A\n",
      " 44%|████▍     | 1941504/4422102 [01:59<02:33, 16136.19it/s]\u001b[A\n",
      " 44%|████▍     | 1949696/4422102 [01:59<02:19, 17736.19it/s]\u001b[A\n",
      " 44%|████▍     | 1957888/4422102 [02:00<02:09, 19027.99it/s]\u001b[A\n",
      " 44%|████▍     | 1966080/4422102 [02:00<02:02, 20033.09it/s]\u001b[A\n",
      " 45%|████▍     | 1974272/4422102 [02:00<02:01, 20193.73it/s]\u001b[A\n",
      " 45%|████▍     | 1982464/4422102 [02:01<02:02, 19846.66it/s]\u001b[A\n",
      " 45%|████▌     | 1990656/4422102 [02:01<02:06, 19239.43it/s]\u001b[A\n",
      " 45%|████▌     | 1998848/4422102 [02:02<01:41, 23907.17it/s]\u001b[A\n",
      " 45%|████▌     | 2007040/4422102 [02:02<01:41, 23695.46it/s]\u001b[A\n",
      " 46%|████▌     | 2015232/4422102 [02:02<01:42, 23508.91it/s]\u001b[A\n",
      " 46%|████▌     | 2023424/4422102 [02:03<02:44, 14542.71it/s]\u001b[A\n",
      " 46%|████▋     | 2056192/4422102 [02:04<02:09, 18303.86it/s]\u001b[A\n",
      " 47%|████▋     | 2064384/4422102 [02:04<02:00, 19499.42it/s]\u001b[A\n",
      " 47%|████▋     | 2072576/4422102 [02:05<01:55, 20417.66it/s]\u001b[A\n",
      " 47%|████▋     | 2080768/4422102 [02:05<01:50, 21151.77it/s]\u001b[A\n",
      " 47%|████▋     | 2088960/4422102 [02:05<01:47, 21696.04it/s]\u001b[A\n",
      " 47%|████▋     | 2097152/4422102 [02:06<01:45, 22068.49it/s]\u001b[A\n",
      " 48%|████▊     | 2105344/4422102 [02:06<01:53, 20485.51it/s]\u001b[A\n",
      " 48%|████▊     | 2113536/4422102 [02:07<02:13, 17330.77it/s]\u001b[A\n",
      " 48%|████▊     | 2121728/4422102 [02:08<02:27, 15610.69it/s]\u001b[A\n",
      " 48%|████▊     | 2129920/4422102 [02:08<03:02, 12558.50it/s]\u001b[A\n",
      " 48%|████▊     | 2138112/4422102 [02:10<03:36, 10552.02it/s]\u001b[A\n",
      " 49%|████▊     | 2146304/4422102 [02:10<03:00, 12605.16it/s]\u001b[A\n",
      " 49%|████▊     | 2154496/4422102 [02:11<03:04, 12264.92it/s]\u001b[A\n",
      " 49%|████▉     | 2162688/4422102 [02:11<02:38, 14260.61it/s]\u001b[A\n",
      " 49%|████▉     | 2170880/4422102 [02:11<02:19, 16108.64it/s]\u001b[A\n",
      " 49%|████▉     | 2179072/4422102 [02:12<02:06, 17693.64it/s]\u001b[A\n",
      " 49%|████▉     | 2187264/4422102 [02:12<01:57, 18995.25it/s]\u001b[A\n",
      " 50%|████▉     | 2195456/4422102 [02:12<01:51, 20054.82it/s]\u001b[A\n",
      " 50%|████▉     | 2203648/4422102 [02:13<02:15, 16407.32it/s]\u001b[A\n",
      " 50%|█████     | 2211840/4422102 [02:13<01:57, 18764.57it/s]\u001b[A\n",
      " 50%|█████     | 2220032/4422102 [02:14<01:56, 18972.68it/s]\u001b[A\n",
      " 50%|█████     | 2228224/4422102 [02:14<01:49, 20039.08it/s]\u001b[A\n",
      " 51%|█████     | 2236416/4422102 [02:15<01:44, 20841.01it/s]\u001b[A\n",
      " 51%|█████     | 2244608/4422102 [02:15<01:41, 21445.34it/s]\u001b[A\n",
      " 51%|█████     | 2252800/4422102 [02:15<01:39, 21886.22it/s]\u001b[A\n",
      " 51%|█████▏    | 2269184/4422102 [02:16<01:22, 25969.31it/s]\u001b[A\n",
      " 51%|█████▏    | 2277376/4422102 [02:16<01:25, 25027.01it/s]\u001b[A\n",
      " 52%|█████▏    | 2285568/4422102 [02:17<02:23, 14894.79it/s]\u001b[A\n",
      " 52%|█████▏    | 2301952/4422102 [02:17<01:53, 18681.18it/s]\u001b[A\n",
      " 52%|█████▏    | 2310144/4422102 [02:18<01:46, 19812.48it/s]\u001b[A\n",
      " 52%|█████▏    | 2318336/4422102 [02:18<01:41, 20664.51it/s]\u001b[A\n",
      " 53%|█████▎    | 2326528/4422102 [02:19<02:05, 16671.23it/s]\u001b[A\n",
      " 53%|█████▎    | 2342912/4422102 [02:20<02:07, 16251.50it/s]\u001b[A\n",
      " 53%|█████▎    | 2359296/4422102 [02:20<01:42, 20171.05it/s]\u001b[A\n",
      " 54%|█████▎    | 2367488/4422102 [02:21<02:04, 16468.35it/s]\u001b[A\n",
      " 54%|█████▎    | 2375680/4422102 [02:22<02:20, 14594.46it/s]\u001b[A\n",
      " 54%|█████▍    | 2383872/4422102 [02:22<02:20, 14548.20it/s]\u001b[A\n",
      " 54%|█████▍    | 2392064/4422102 [02:23<03:10, 10641.59it/s]\u001b[A\n",
      " 54%|█████▍    | 2400256/4422102 [02:24<03:05, 10891.46it/s]\u001b[A\n",
      " 54%|█████▍    | 2408448/4422102 [02:25<03:28, 9677.35it/s] \u001b[A\n",
      " 55%|█████▍    | 2416640/4422102 [02:26<02:51, 11718.17it/s]\u001b[A\n",
      " 55%|█████▍    | 2424832/4422102 [02:26<02:51, 11665.26it/s]\u001b[A\n",
      " 55%|█████▌    | 2433024/4422102 [02:27<02:28, 13391.19it/s]\u001b[A\n",
      " 55%|█████▌    | 2441216/4422102 [02:27<02:09, 15303.12it/s]\u001b[A\n",
      " 55%|█████▌    | 2449408/4422102 [02:28<02:18, 14252.02it/s]\u001b[A\n",
      " 56%|█████▌    | 2457600/4422102 [02:28<02:05, 15672.83it/s]\u001b[A\n",
      " 56%|█████▌    | 2465792/4422102 [02:29<01:52, 17334.81it/s]\u001b[A\n",
      " 56%|█████▌    | 2473984/4422102 [02:29<02:09, 15015.71it/s]\u001b[A\n",
      " 56%|█████▋    | 2490368/4422102 [02:30<01:44, 18499.62it/s]\u001b[A\n",
      " 57%|█████▋    | 2498560/4422102 [02:30<01:34, 20373.60it/s]\u001b[A\n",
      " 57%|█████▋    | 2506752/4422102 [02:30<01:30, 21052.66it/s]\u001b[A\n",
      " 57%|█████▋    | 2514944/4422102 [02:31<01:53, 16875.37it/s]\u001b[A\n",
      " 57%|█████▋    | 2523136/4422102 [02:31<01:43, 18297.97it/s]\u001b[A\n",
      " 57%|█████▋    | 2531328/4422102 [02:32<01:36, 19512.52it/s]\u001b[A\n",
      " 57%|█████▋    | 2539520/4422102 [02:32<01:56, 16152.36it/s]\u001b[A\n",
      " 58%|█████▊    | 2555904/4422102 [02:33<01:33, 20021.76it/s]\u001b[A\n",
      " 58%|█████▊    | 2564096/4422102 [02:33<01:29, 20777.19it/s]\u001b[A\n",
      " 58%|█████▊    | 2572288/4422102 [02:34<01:32, 20064.02it/s]\u001b[A\n",
      " 58%|█████▊    | 2580480/4422102 [02:34<01:52, 16363.31it/s]\u001b[A\n",
      " 59%|█████▊    | 2588672/4422102 [02:35<01:42, 17928.64it/s]\u001b[A\n",
      " 59%|█████▊    | 2596864/4422102 [02:35<01:35, 19211.02it/s]\u001b[A\n",
      " 59%|█████▉    | 2605056/4422102 [02:35<01:29, 20229.80it/s]\u001b[A\n",
      " 59%|█████▉    | 2613248/4422102 [02:36<01:26, 21016.87it/s]\u001b[A\n",
      " 59%|█████▉    | 2621440/4422102 [02:36<01:23, 21591.89it/s]\u001b[A\n",
      " 59%|█████▉    | 2629632/4422102 [02:36<01:21, 21983.12it/s]\u001b[A\n",
      " 60%|█████▉    | 2637824/4422102 [02:37<01:19, 22326.45it/s]\u001b[A\n",
      " 60%|█████▉    | 2646016/4422102 [02:37<01:18, 22538.11it/s]\u001b[A\n",
      " 60%|██████    | 2654208/4422102 [02:38<01:17, 22688.46it/s]\u001b[A\n",
      " 60%|██████    | 2662400/4422102 [02:38<01:55, 15294.70it/s]\u001b[A\n",
      " 60%|██████    | 2670592/4422102 [02:39<01:51, 15657.73it/s]\u001b[A\n",
      " 61%|██████    | 2686976/4422102 [02:40<01:35, 18113.07it/s]\u001b[A\n",
      " 61%|██████    | 2695168/4422102 [02:40<01:29, 19351.67it/s]\u001b[A\n",
      " 61%|██████    | 2703360/4422102 [02:40<01:24, 20332.04it/s]\u001b[A\n",
      " 61%|██████▏   | 2711552/4422102 [02:41<01:21, 21073.56it/s]\u001b[A\n",
      " 62%|██████▏   | 2719744/4422102 [02:41<01:18, 21570.12it/s]\u001b[A\n",
      " 62%|██████▏   | 2727936/4422102 [02:41<01:17, 21971.80it/s]\u001b[A\n",
      " 62%|██████▏   | 2736128/4422102 [02:42<01:15, 22287.04it/s]\u001b[A\n",
      " 62%|██████▏   | 2744320/4422102 [02:42<01:01, 27442.80it/s]\u001b[A\n",
      " 62%|██████▏   | 2752512/4422102 [02:43<01:44, 15996.54it/s]\u001b[A\n",
      " 63%|██████▎   | 2768896/4422102 [02:44<01:35, 17303.91it/s]\u001b[A\n",
      " 63%|██████▎   | 2777088/4422102 [02:44<01:27, 18695.92it/s]\u001b[A\n",
      " 63%|██████▎   | 2785280/4422102 [02:45<01:43, 15762.57it/s]\u001b[A\n",
      " 63%|██████▎   | 2793472/4422102 [02:45<01:33, 17393.45it/s]\u001b[A\n",
      " 63%|██████▎   | 2801664/4422102 [02:45<01:26, 18740.96it/s]\u001b[A\n",
      " 64%|██████▎   | 2809856/4422102 [02:46<01:21, 19860.66it/s]\u001b[A\n",
      " 64%|██████▎   | 2818048/4422102 [02:47<02:02, 13115.69it/s]\u001b[A\n",
      " 64%|██████▍   | 2834432/4422102 [02:47<01:44, 15230.27it/s]\u001b[A\n",
      " 64%|██████▍   | 2842624/4422102 [02:48<01:33, 16885.42it/s]\u001b[A\n",
      " 64%|██████▍   | 2850816/4422102 [02:49<02:08, 12181.13it/s]\u001b[A\n",
      " 65%|██████▍   | 2859008/4422102 [02:49<01:50, 14171.20it/s]\u001b[A\n",
      " 65%|██████▍   | 2867200/4422102 [02:50<01:37, 16029.30it/s]\u001b[A\n",
      " 65%|██████▌   | 2875392/4422102 [02:50<01:27, 17641.61it/s]\u001b[A\n",
      " 65%|██████▌   | 2883584/4422102 [02:51<01:41, 15175.62it/s]\u001b[A\n",
      " 66%|██████▌   | 2899968/4422102 [02:51<01:20, 18989.09it/s]\u001b[A\n",
      " 66%|██████▌   | 2908160/4422102 [02:51<01:15, 20043.54it/s]\u001b[A\n",
      " 66%|██████▌   | 2916352/4422102 [02:52<01:24, 17803.16it/s]\u001b[A\n",
      " 66%|██████▌   | 2924544/4422102 [02:52<01:06, 22629.25it/s]\u001b[A\n",
      " 66%|██████▋   | 2932736/4422102 [02:53<01:05, 22687.45it/s]\u001b[A\n",
      " 67%|██████▋   | 2940928/4422102 [02:53<01:04, 22799.71it/s]\u001b[A\n",
      " 67%|██████▋   | 2949120/4422102 [02:53<01:04, 22885.49it/s]\u001b[A\n",
      " 67%|██████▋   | 2957312/4422102 [02:54<01:03, 22918.82it/s]\u001b[A\n",
      " 67%|██████▋   | 2965504/4422102 [02:54<01:01, 23660.64it/s]\u001b[A\n",
      " 67%|██████▋   | 2973696/4422102 [02:54<01:01, 23479.21it/s]\u001b[A\n",
      " 67%|██████▋   | 2981888/4422102 [02:55<01:20, 17918.59it/s]\u001b[A\n",
      " 68%|██████▊   | 2990080/4422102 [02:55<01:14, 19205.89it/s]\u001b[A\n",
      " 68%|██████▊   | 2998272/4422102 [02:56<01:10, 20184.32it/s]\u001b[A\n",
      " 68%|██████▊   | 3006464/4422102 [02:56<01:26, 16433.66it/s]\u001b[A\n",
      " 68%|██████▊   | 3022848/4422102 [02:57<01:11, 19613.89it/s]\u001b[A\n",
      " 69%|██████▊   | 3031040/4422102 [02:57<01:08, 20397.85it/s]\u001b[A\n",
      " 69%|██████▊   | 3039232/4422102 [02:57<01:00, 22824.11it/s]\u001b[A\n",
      " 69%|██████▉   | 3047424/4422102 [02:58<01:00, 22908.55it/s]\u001b[A\n",
      " 69%|██████▉   | 3055616/4422102 [02:58<00:59, 22919.90it/s]\u001b[A\n",
      " 69%|██████▉   | 3063808/4422102 [02:59<00:59, 22853.96it/s]\u001b[A\n",
      " 69%|██████▉   | 3072000/4422102 [02:59<00:46, 29087.87it/s]\u001b[A\n",
      " 70%|██████▉   | 3080192/4422102 [02:59<00:49, 26956.31it/s]\u001b[A\n",
      " 70%|██████▉   | 3088384/4422102 [02:59<00:47, 28292.57it/s]\u001b[A\n",
      " 70%|███████   | 3096576/4422102 [03:00<00:50, 26468.51it/s]\u001b[A\n",
      " 70%|███████   | 3104768/4422102 [03:00<01:09, 19050.35it/s]\u001b[A\n",
      " 71%|███████   | 3121152/4422102 [03:01<01:08, 19089.44it/s]\u001b[A\n",
      " 71%|███████   | 3129344/4422102 [03:01<00:57, 22475.18it/s]\u001b[A\n",
      " 71%|███████   | 3137536/4422102 [03:02<00:56, 22566.03it/s]\u001b[A\n",
      " 71%|███████   | 3145728/4422102 [03:02<00:59, 21518.38it/s]\u001b[A\n",
      " 71%|███████▏  | 3153920/4422102 [03:03<01:03, 19997.22it/s]\u001b[A\n",
      " 72%|███████▏  | 3162112/4422102 [03:03<01:08, 18426.83it/s]\u001b[A\n",
      " 72%|███████▏  | 3170304/4422102 [03:04<01:20, 15609.43it/s]\u001b[A\n",
      " 72%|███████▏  | 3186688/4422102 [03:04<01:07, 18242.21it/s]\u001b[A\n",
      " 72%|███████▏  | 3194880/4422102 [03:05<01:19, 15484.26it/s]\u001b[A\n",
      " 72%|███████▏  | 3203072/4422102 [03:06<01:22, 14740.88it/s]\u001b[A\n",
      " 73%|███████▎  | 3211264/4422102 [03:07<01:37, 12399.74it/s]\u001b[A\n",
      " 73%|███████▎  | 3219456/4422102 [03:07<01:23, 14389.64it/s]\u001b[A\n",
      " 73%|███████▎  | 3227648/4422102 [03:09<02:11, 9107.20it/s] \u001b[A\n",
      " 73%|███████▎  | 3235840/4422102 [03:10<02:18, 8572.64it/s]\u001b[A\n",
      " 73%|███████▎  | 3244032/4422102 [03:11<02:07, 9271.11it/s]\u001b[A\n",
      " 74%|███████▎  | 3252224/4422102 [03:11<01:59, 9814.13it/s]\u001b[A\n",
      " 74%|███████▎  | 3260416/4422102 [03:12<02:08, 9048.02it/s]\u001b[A\n",
      " 74%|███████▍  | 3268608/4422102 [03:13<01:59, 9671.19it/s]\u001b[A\n",
      " 74%|███████▍  | 3276800/4422102 [03:13<01:39, 11498.37it/s]\u001b[A\n",
      " 74%|███████▍  | 3284992/4422102 [03:14<01:37, 11718.55it/s]\u001b[A\n",
      " 74%|███████▍  | 3293184/4422102 [03:14<01:23, 13461.19it/s]\u001b[A\n",
      " 75%|███████▍  | 3301376/4422102 [03:15<01:12, 15378.65it/s]\u001b[A\n",
      " 75%|███████▍  | 3309568/4422102 [03:16<01:17, 14279.11it/s]\u001b[A\n",
      " 75%|███████▌  | 3325952/4422102 [03:16<01:02, 17449.99it/s]\u001b[A\n",
      " 75%|███████▌  | 3334144/4422102 [03:16<00:57, 18784.47it/s]\u001b[A\n",
      " 76%|███████▌  | 3342336/4422102 [03:17<01:22, 13020.36it/s]\u001b[A\n",
      " 76%|███████▌  | 3366912/4422102 [03:18<01:01, 17199.53it/s]\u001b[A\n",
      " 76%|███████▋  | 3375104/4422102 [03:18<01:07, 15427.16it/s]\u001b[A\n",
      " 77%|███████▋  | 3391488/4422102 [03:19<00:53, 19242.20it/s]\u001b[A\n",
      " 77%|███████▋  | 3399680/4422102 [03:19<00:50, 20131.81it/s]\u001b[A\n",
      " 77%|███████▋  | 3407872/4422102 [03:19<00:48, 20933.73it/s]\u001b[A\n",
      " 77%|███████▋  | 3416064/4422102 [03:20<00:43, 22962.29it/s]\u001b[A\n",
      " 77%|███████▋  | 3424256/4422102 [03:20<00:38, 26242.45it/s]\u001b[A\n",
      " 78%|███████▊  | 3432448/4422102 [03:20<00:34, 28482.12it/s]\u001b[A\n",
      " 78%|███████▊  | 3440640/4422102 [03:21<00:49, 19741.19it/s]\u001b[A\n",
      " 78%|███████▊  | 3448832/4422102 [03:21<00:41, 23446.75it/s]\u001b[A\n",
      " 78%|███████▊  | 3457024/4422102 [03:22<00:56, 16994.73it/s]\u001b[A\n",
      " 79%|███████▊  | 3473408/4422102 [03:22<00:46, 20273.32it/s]\u001b[A\n",
      " 79%|███████▊  | 3481600/4422102 [03:23<00:44, 20983.37it/s]\u001b[A\n",
      " 79%|███████▉  | 3489792/4422102 [03:24<01:09, 13461.57it/s]\u001b[A\n",
      " 79%|███████▉  | 3497984/4422102 [03:24<00:58, 15795.48it/s]\u001b[A\n",
      " 79%|███████▉  | 3506176/4422102 [03:24<00:52, 17432.60it/s]\u001b[A\n",
      " 79%|███████▉  | 3514368/4422102 [03:25<00:55, 16211.38it/s]\u001b[A\n",
      " 80%|███████▉  | 3522560/4422102 [03:25<00:50, 17783.68it/s]\u001b[A\n",
      " 80%|███████▉  | 3530752/4422102 [03:26<00:50, 17588.74it/s]\u001b[A\n",
      " 80%|████████  | 3538944/4422102 [03:26<00:46, 18937.38it/s]\u001b[A\n",
      " 80%|████████  | 3547136/4422102 [03:27<00:51, 17080.62it/s]\u001b[A\n",
      " 80%|████████  | 3555328/4422102 [03:27<00:39, 21999.65it/s]\u001b[A\n",
      " 81%|████████  | 3563520/4422102 [03:27<00:38, 22310.61it/s]\u001b[A\n",
      " 81%|████████  | 3571712/4422102 [03:28<00:37, 22523.21it/s]\u001b[A\n",
      " 81%|████████  | 3579904/4422102 [03:28<00:33, 25210.92it/s]\u001b[A\n",
      " 81%|████████  | 3588096/4422102 [03:28<00:34, 24470.84it/s]\u001b[A\n",
      " 81%|████████▏ | 3596288/4422102 [03:28<00:27, 30209.11it/s]\u001b[A\n",
      " 82%|████████▏ | 3604480/4422102 [03:29<00:25, 31556.10it/s]\u001b[A\n",
      " 82%|████████▏ | 3620864/4422102 [03:29<00:28, 28354.45it/s]\u001b[A\n",
      " 83%|████████▎ | 3653632/4422102 [03:30<00:26, 28879.90it/s]\u001b[A\n",
      " 84%|████████▎ | 3694592/4422102 [03:31<00:19, 37231.32it/s]\u001b[A\n",
      " 84%|████████▎ | 3702784/4422102 [03:31<00:22, 31334.81it/s]\u001b[A\n",
      " 84%|████████▍ | 3719168/4422102 [03:32<00:20, 34667.79it/s]\u001b[A\n",
      " 84%|████████▍ | 3735552/4422102 [03:32<00:18, 37444.67it/s]\u001b[A\n",
      " 85%|████████▍ | 3751936/4422102 [03:33<00:21, 31240.57it/s]\u001b[A\n",
      " 85%|████████▌ | 3776512/4422102 [03:33<00:17, 37410.78it/s]\u001b[A\n",
      " 86%|████████▌ | 3784704/4422102 [03:33<00:20, 31581.26it/s]\u001b[A\n",
      " 86%|████████▌ | 3801088/4422102 [03:34<00:22, 28176.34it/s]\u001b[A\n",
      " 87%|████████▋ | 3825664/4422102 [03:34<00:17, 34285.73it/s]\u001b[A\n",
      " 87%|████████▋ | 3833856/4422102 [03:35<00:27, 21480.21it/s]\u001b[A\n",
      " 87%|████████▋ | 3842048/4422102 [03:35<00:26, 21903.30it/s]\u001b[A\n",
      " 87%|████████▋ | 3858432/4422102 [03:36<00:27, 20693.95it/s]\u001b[A\n",
      " 88%|████████▊ | 3874816/4422102 [03:37<00:22, 24790.77it/s]\u001b[A\n",
      " 88%|████████▊ | 3883008/4422102 [03:37<00:29, 18434.34it/s]\u001b[A\n",
      " 88%|████████▊ | 3891200/4422102 [03:38<00:27, 19611.38it/s]\u001b[A\n",
      " 88%|████████▊ | 3899392/4422102 [03:38<00:25, 20536.51it/s]\u001b[A\n",
      " 88%|████████▊ | 3907584/4422102 [03:38<00:24, 21233.38it/s]\u001b[A\n",
      " 89%|████████▊ | 3915776/4422102 [03:39<00:23, 21728.14it/s]\u001b[A\n",
      " 89%|████████▊ | 3923968/4422102 [03:39<00:22, 22087.23it/s]\u001b[A\n",
      " 89%|████████▉ | 3932160/4422102 [03:40<00:21, 22371.64it/s]\u001b[A\n",
      " 89%|████████▉ | 3940352/4422102 [03:40<00:21, 22578.54it/s]\u001b[A\n",
      " 89%|████████▉ | 3948544/4422102 [03:40<00:20, 22685.53it/s]\u001b[A\n",
      " 89%|████████▉ | 3956736/4422102 [03:41<00:26, 17528.74it/s]\u001b[A\n",
      " 90%|████████▉ | 3973120/4422102 [03:42<00:23, 18865.84it/s]\u001b[A\n",
      " 90%|█████████ | 3981312/4422102 [03:42<00:27, 15845.29it/s]\u001b[A\n",
      " 90%|█████████ | 3989504/4422102 [03:43<00:30, 14204.69it/s]\u001b[A\n",
      " 90%|█████████ | 3997696/4422102 [03:43<00:26, 16059.43it/s]\u001b[A\n",
      " 91%|█████████ | 4005888/4422102 [03:44<00:23, 17640.46it/s]\u001b[A\n",
      " 91%|█████████ | 4014080/4422102 [03:44<00:21, 18898.29it/s]\u001b[A\n",
      " 91%|█████████ | 4022272/4422102 [03:45<00:20, 19933.59it/s]\u001b[A\n",
      " 91%|█████████ | 4030464/4422102 [03:45<00:18, 20758.16it/s]\u001b[A\n",
      " 91%|█████████▏| 4038656/4422102 [03:45<00:17, 21396.16it/s]\u001b[A\n",
      " 92%|█████████▏| 4046848/4422102 [03:46<00:22, 17014.16it/s]\u001b[A\n",
      " 92%|█████████▏| 4063232/4422102 [03:48<00:25, 13989.19it/s]\u001b[A\n",
      " 92%|█████████▏| 4071424/4422102 [03:48<00:26, 13140.06it/s]\u001b[A\n",
      " 92%|█████████▏| 4079616/4422102 [03:49<00:22, 15084.85it/s]\u001b[A\n",
      " 92%|█████████▏| 4087808/4422102 [03:49<00:20, 16397.37it/s]\u001b[A\n",
      " 93%|█████████▎| 4096000/4422102 [03:50<00:21, 14844.78it/s]\u001b[A\n",
      " 93%|█████████▎| 4104192/4422102 [03:50<00:19, 16583.41it/s]\u001b[A\n",
      " 93%|█████████▎| 4112384/4422102 [03:51<00:21, 14653.75it/s]\u001b[A\n",
      " 93%|█████████▎| 4120576/4422102 [03:51<00:17, 17239.33it/s]\u001b[A\n",
      " 93%|█████████▎| 4128768/4422102 [03:52<00:18, 15757.78it/s]\u001b[A\n",
      " 94%|█████████▎| 4136960/4422102 [03:52<00:20, 13927.27it/s]\u001b[A\n",
      " 94%|█████████▎| 4145152/4422102 [03:53<00:17, 15805.29it/s]\u001b[A\n",
      " 94%|█████████▍| 4153344/4422102 [03:53<00:15, 17434.69it/s]\u001b[A\n",
      " 94%|█████████▍| 4161536/4422102 [03:54<00:13, 18728.68it/s]\u001b[A\n",
      " 94%|█████████▍| 4169728/4422102 [03:55<00:19, 13065.36it/s]\u001b[A\n",
      " 94%|█████████▍| 4177920/4422102 [03:55<00:15, 15909.04it/s]\u001b[A\n",
      " 95%|█████████▍| 4186112/4422102 [03:55<00:14, 16370.50it/s]\u001b[A\n",
      " 95%|█████████▍| 4194304/4422102 [03:56<00:11, 19280.66it/s]\u001b[A\n",
      " 95%|█████████▌| 4202496/4422102 [03:56<00:10, 20273.31it/s]\u001b[A\n",
      " 95%|█████████▌| 4210688/4422102 [03:56<00:10, 21000.43it/s]\u001b[A\n",
      " 95%|█████████▌| 4218880/4422102 [03:57<00:09, 21489.92it/s]\u001b[A\n",
      " 96%|█████████▌| 4227072/4422102 [03:57<00:08, 21936.95it/s]\u001b[A\n",
      " 96%|█████████▌| 4235264/4422102 [03:57<00:06, 27918.12it/s]\u001b[A\n",
      " 96%|█████████▌| 4243456/4422102 [03:57<00:06, 26245.04it/s]\u001b[A\n",
      " 96%|█████████▌| 4251648/4422102 [03:58<00:06, 25148.44it/s]\u001b[A\n",
      " 96%|█████████▋| 4259840/4422102 [03:58<00:05, 27067.73it/s]\u001b[A\n",
      " 97%|█████████▋| 4268032/4422102 [03:58<00:04, 31997.40it/s]\u001b[A\n",
      " 97%|█████████▋| 4276224/4422102 [03:59<00:04, 29933.68it/s]\u001b[A\n",
      " 97%|█████████▋| 4284416/4422102 [03:59<00:04, 30767.82it/s]\u001b[A\n",
      " 97%|█████████▋| 4292608/4422102 [03:59<00:03, 37501.28it/s]\u001b[A\n",
      " 97%|█████████▋| 4300800/4422102 [04:00<00:05, 22380.53it/s]\u001b[A\n",
      " 98%|█████████▊| 4333568/4422102 [04:00<00:03, 28959.76it/s]\u001b[A\n",
      " 98%|█████████▊| 4349952/4422102 [04:00<00:02, 32597.71it/s]\u001b[A\n",
      " 99%|█████████▊| 4366336/4422102 [04:01<00:01, 35743.51it/s]\u001b[A\n",
      " 99%|█████████▉| 4382720/4422102 [04:01<00:01, 38327.94it/s]\u001b[A\n",
      "100%|█████████▉| 4407296/4422102 [04:01<00:00, 44243.19it/s]\u001b[A\n",
      "4423680it [04:02, 46308.66it/s]                             \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5148 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "8192it [00:01, 6899.96it/s]             \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/kesci/Datasets/FashionMNIST/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4423680it [04:21, 46308.66it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "mnist_train = torchvision.datasets.FashionMNIST(root='/home/kesci/Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='/home/kesci/Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_mekh927",
    "id": "DD5743BBD5614F49829E2AA08F422D77",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)\n",
    "- root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。\n",
    "- train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。\n",
    "- download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。\n",
    "- transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。\n",
    "- target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_d4dkkil",
    "id": "B4B6EB3C4CF74C1D8034A9A349870804",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.FashionMNIST'>\n",
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "# show result \n",
    "print(type(mnist_train))\n",
    "print(len(mnist_train), len(mnist_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_yndoddn",
    "id": "E35B37B4A94A44AA89722155E8AB5788",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 9\n"
     ]
    }
   ],
   "source": [
    "# 我们可以通过下标来访问任意一个样本\n",
    "feature, label = mnist_train[0]\n",
    "print(feature.shape, label)  # Channel x Height x Width\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_lkp5d0p",
    "id": "AFC6C721591743D789E443EE6637458E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如果不做变换输入的数据是图像，我们可以看一下图片的类型参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_2dk2d0m",
    "id": "C72CDF6B5BA64F36AAB028CFCB1242C9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=28x28 at 0x7F1CD8B69C88>\n"
     ]
    }
   ],
   "source": [
    "mnist_PIL = torchvision.datasets.FashionMNIST(root='/home/kesci/Datasets/FashionMNIST', train=True, download=True)\n",
    "PIL_feature, label = mnist_PIL[0]\n",
    "print(PIL_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_pku7gp5",
    "id": "B7454582B1484EE4B227BF145B0ED3BA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_1x2gagm",
    "id": "D241CBCCD1CD47F28B9480080B93D4FD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_fashion_mnist(images, labels):\n",
    "    d2l.use_svg_display()\n",
    "    # 这里的_表示我们忽略（不使用）的变量\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.view((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_us9fuso",
    "id": "056F457B00454FFD81A3CB6AD966C508",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/056F457B00454FFD81A3CB6AD966C508/q5lfhyw7sb.svg\">"
      ],
      "text/plain": [
       "<Figure size 864x864 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for i in range(10):\n",
    "    X.append(mnist_train[i][0]) # 将第i个feature加到X中\n",
    "    y.append(mnist_train[i][1]) # 将第i个label加到y中\n",
    "show_fashion_mnist(X, get_fashion_mnist_labels(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "graffitiCellId": "id_uy1hu22",
    "id": "34AD2BCBAD9149428E1C29B6F5530BAC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "graffitiCellId": "id_2vdyxtx",
    "id": "2A439C938A5B43449F0160F5DACB4759",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.95 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "print('%.2f sec' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_0nzlmif",
    "id": "7886893A27B44B979B7E4B93F870B3CE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# softmax从零开始的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_c0aor6l",
    "id": "C47E4519B73C46558F23670B6C6D2CF4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "0.4.1a0+d94043a\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\")\n",
    "import d2lzh1981 as d2l\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_63x3cw1",
    "id": "24E441B54DB5429991AC9DEBB6014FEC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 获取训练集数据和测试集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "graffitiCellId": "id_lkxl1n6",
    "id": "39EFD4C2466B4A649B15C7535800FAD6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_h6qpe01",
    "id": "7DC915895B5C46278F4C395BD49815A0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 模型参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "graffitiCellId": "id_xcafpiw",
    "id": "FD4BCCEF1A044DF99791FB17D8134B6A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 784\n",
    "print(28*28)\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\n",
    "b = torch.zeros(num_outputs, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "graffitiCellId": "id_1ow0q3n",
    "id": "D5372FA087BA4BAF87E940582444FB2D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_d5s0vs0",
    "id": "3BBF52F8DB29448B9D44F5D707596788",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 对多维Tensor按维度操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "graffitiCellId": "id_e13vki6",
    "id": "6BFBFFFB54E44D05B1FC16BE841BB815",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n",
      "tensor([[ 6],\n",
      "        [15]])\n",
      "tensor([5, 7, 9])\n",
      "tensor([ 6, 15])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征\n",
    "print(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征\n",
    "print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征\n",
    "print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_fn39y6u",
    "id": "F15E94B777BA4E9F88CB18BD16697A45",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义softmax操作\n",
    "\n",
    "$$\n",
    " \\hat{y}_j = \\frac{ \\exp(o_j)}{\\sum_{i=1}^3 \\exp(o_i)} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "graffitiCellId": "id_cr8iqvr",
    "id": "7F882A63347F46318C5F334F4F55F123",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    # print(\"X size is \", X_exp.size())\n",
    "    # print(\"partition size is \", partition, partition.size())\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "graffitiCellId": "id_vxj92pl",
    "id": "A77A982B8916433AAB521914CFD8467D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1602, 0.2785, 0.1890, 0.2279, 0.1444],\n",
      "        [0.1451, 0.2192, 0.2549, 0.1246, 0.2562]]) \n",
      " tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((2, 5))\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, '\\n', X_prob.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bkbmqh1",
    "id": "F59CC29AE6F5407A84064647281E9ECA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## softmax回归模型\n",
    " \n",
    "$$\n",
    " \\begin{aligned} \\boldsymbol{o}^{(i)} &= \\boldsymbol{x}^{(i)} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{y}}^{(i)} &= \\text{softmax}(\\boldsymbol{o}^{(i)}). \\end{aligned} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "graffitiCellId": "id_76a2fpk",
    "id": "0B3F76DCDD23439D849A2AE842D7DBE5",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9kc1jw8",
    "id": "5024E97053FF460A8CB5F2FCC75C0C27",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义损失函数\n",
    "\n",
    "$$\n",
    "H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},\n",
    "$$\n",
    "  \n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ),\n",
    "$$\n",
    "  \n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\Theta}) = -(1/n) \\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)}\n",
    "$$\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "graffitiCellId": "id_fl3cdvq",
    "id": "0244A13F3B4F43EA9B8D2FC5800238C7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.5000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = torch.LongTensor([0, 2])\n",
    "y_hat.gather(1, y.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "graffitiCellId": "id_4ymtpzn",
    "id": "D288CFCAE8DF464287ECA08AD5168F2E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_epbrym1",
    "id": "83DD3AF7D13F489D8683CB751F430CBF",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义准确率\n",
    "    我们模型训练完了进行模型预测的时候，会用到我们这里定义的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "graffitiCellId": "id_8h5q5ic",
    "id": "B82A0C9B1F984CF089B9FA8747BE1A3B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "graffitiCellId": "id_2szlvh2",
    "id": "9A6EF0B8618B400D8D252223B6B04E2D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "graffitiCellId": "id_gtpf1ob",
    "id": "963FEE28FD23446895193E95A4915F2A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "graffitiCellId": "id_90h5xb7",
    "id": "BDDE1D7C593D47B78465BA7E4F991B82",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0671\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_accuracy(test_iter, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_1p8f5ji",
    "id": "A209780170A64CC29271427320284785",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "graffitiCellId": "id_smslnb6",
    "id": "EFDCDFDDE4B44FA3866BEABC48F44D57",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7841, train acc 0.749, test acc 0.792\n",
      "epoch 2, loss 0.5705, train acc 0.813, test acc 0.812\n",
      "epoch 3, loss 0.5254, train acc 0.825, test acc 0.818\n",
      "epoch 4, loss 0.5005, train acc 0.833, test acc 0.825\n",
      "epoch 5, loss 0.4853, train acc 0.837, test acc 0.828\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            \n",
    "            \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_eptwt05",
    "id": "D7885C25CD7444BF8212D2102E99692C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 模型预测\n",
    "现在我们的模型训练完了，可以进行一下预测，我们的这个模型训练的到底准确不准确。\n",
    "现在就可以演示如何对图像进行分类了。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "graffitiCellId": "id_yt8sd9t",
    "id": "1DA8927186304BEBA2B3DCC4A9E027DD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/1DA8927186304BEBA2B3DCC4A9E027DD/q5lfm1bok2.svg\">"
      ],
      "text/plain": [
       "<Figure size 864x864 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = iter(test_iter).next()\n",
    "\n",
    "true_labels = d2l.get_fashion_mnist_labels(y.numpy())\n",
    "pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())\n",
    "titles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n",
    "\n",
    "d2l.show_fashion_mnist(X[0:9], titles[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_623cj7f",
    "id": "740E43F6246C47078E68E5EB4399345A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# softmax的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "graffitiCellId": "id_s0m2c8a",
    "id": "94B0411C93ED4E6C83B5545D9339E395",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# 加载各种包或者模块\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\")\n",
    "import d2lzh1981 as d2l\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_7ny2q9r",
    "id": "21E9BF3FC62E42FAA0D601526BC27572",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 初始化参数和获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "graffitiCellId": "id_3ra54mv",
    "id": "08E33ED9A5684635874D11BFC0B488C6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_pvkd7mm",
    "id": "7BCB961EF5464E1980619F1853FE2F84",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "graffitiCellId": "id_1s76ktf",
    "id": "01609B4BD2B444588B2C33751B48B9B0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "    def forward(self, x): # x 的形状: (batch, 1, 28, 28)\n",
    "        y = self.linear(x.view(x.shape[0], -1))\n",
    "        return y\n",
    "    \n",
    "# net = LinearNet(num_inputs, num_outputs)\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x): # x 的形状: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(\n",
    "        # FlattenLayer(),\n",
    "        # LinearNet(num_inputs, num_outputs) \n",
    "        OrderedDict([\n",
    "           ('flatten', FlattenLayer()),\n",
    "           ('linear', nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_nssr12v",
    "id": "1C3A731775674DDFB88CA79FBCFABCFC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "graffitiCellId": "id_id7oqtn",
    "id": "444F87CD78CC4DCE9E76E56B9D096E48",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "init.constant_(net.linear.bias, val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hn4au8n",
    "id": "EC61B11479EA443E812CCF03CC85D871",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "graffitiCellId": "id_knh2n50",
    "id": "EFA9FA88D866458F913194664286D8AB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss() # 下面是他的函数原型\n",
    "# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bdskdx7",
    "id": "FFD0D8B3BB1B4F6582610743373C576E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "graffitiCellId": "id_6l357pq",
    "id": "7DF366DFD7D349B59B0CA9D3211EB140",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 下面是函数原型\n",
    "# class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_kzma3d6",
    "id": "6A660AA6E71541EA85B4955AC0DCC66C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "graffitiCellId": "id_0puuqtc",
    "id": "A49C9427C0E14DB9914E8970D98C422B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.748, test acc 0.790\n",
      "epoch 2, loss 0.0022, train acc 0.814, test acc 0.774\n",
      "epoch 3, loss 0.0021, train acc 0.825, test acc 0.817\n",
      "epoch 4, loss 0.0020, train acc 0.832, test acc 0.824\n",
      "epoch 5, loss 0.0019, train acc 0.836, test acc 0.817\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内容补充，来自课程各位同学评论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax公式的得出方法大概解释可以解释为：\n",
    "\n",
    "首先假设样本与理论标准函数的误差（类似于线性回归那一章中生成数据时叠加上的高斯误差）服从正态分布（高斯分布），并且不同样本之间独立同分布，\n",
    "通过贝叶斯公式计算各个分类的概率，将高斯分布的公式带入公式之后化简得到。\n",
    "在一些地方softmax函数又被称为归一化指数（normalized exponential）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.boyuai.com/elites/course/cZu18YmweLv10OeV/video/-m1RzLMiaJHiHvnuIWFwc#comment-IGQhzLPO0PZnwDrqQK6YL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵\n",
    "下面是根据知乎的一篇文章进行的整理，由于平台的latex好像不支持中文，所以直接将自己笔记的截图放上来了，希望对大家的理解有帮助\n",
    "评论图片\n",
    "﻿\n",
    "评论图片\n",
    "参考链接：https://zhuanlan.zhihu.com/p/35709485"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习评论中的要素整理：\n",
    "\n",
    "Softmax运算（即数值归一化得到0-1概率，通过exp函数，从直接线性化的hard max变为e指数化的soft max，能够使原有的差异倍数变得更加明显）、交叉熵误差函数（因为2范数太过于严格了，softmax只聚焦于最大概率值，而非全值根据熵增的概念对像素分类）。\n",
    "补充：反向传递求梯度前一定要梯度清零，以免累增。\n",
    "首先初始化梯度，计算完一次梯度，更新完之后，清零梯度，进行下一次的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于softmax函数选择\n",
    "看完代码，明显softmax和其他模型最与众不同的特点就是softmax函数了，那么为什么选择softmax函数呢？\n",
    "softmax函数是来自于sigmoid函数在多分类情况下的推广，他们的相同之处：\n",
    "1. 都具有良好的数据压缩能力是实数域R→[ 0 , 1 ]的映射函数，可以将杂乱无序没有实际含义的数字直接转化为每个分类的可能性概率。\n",
    "2. 都具有非常漂亮的导数形式，便于反向传播计算。\n",
    "3. 它们都是 soft version of max ，都可以将数据的差异明显化。\n",
    "\n",
    "相同的，他们具有着不同的特点，sigmoid函数可以看成softmax函数的特例，softmax函数也可以看作sigmoid函数的推广。\n",
    "\n",
    "1. sigmoid函数前提假设是样本服从伯努利 (Bernoulli) 分布的假设，而softmax则是基于多项式分布。首先证明多项分布属于指数分布族，这样就可以使用广义线性模型来拟合这个多项分布，由广义线性模型推导出的目标函数即为Softmax回归的分类模型。 \n",
    "\n",
    "2. sigmoid函数用于分辨每一种情况的可能性，所以用sigmoid函数实现多分类问题的时候，概率并不是归一的，反映的是每个情况的发生概率，因此非互斥的问题使用sigmoid函数可以获得比较漂亮的结果；softmax函数最初的设计思路适用于首先数字识别这样的互斥的多分类问题，因此进行了归一化操作，使得最后预测的结果是唯一的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax的优缺点：\n",
    "\n",
    " https://wmathor.com/index.php/archives/1357/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵我是看了下这篇博客  \n",
    "感觉讲得很好https://blog.csdn.net/b1055077005/article/details/100152102\n",
    "\n",
    "楼上的答案中https://www.boyuai.com/elites/course/cZu18YmweLv10OeV/video/-m1RzLMiaJHiHvnuIWFwc#comment-pZ2avf6zp6ckXO3GQ1zQP 也有相关的内容 可以作为补充\n",
    "\n",
    "后面关于softmax函数的可以参考下这篇文章https://blog.csdn.net/bitcarmanlee/article/details/82320853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
